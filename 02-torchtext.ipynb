{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data import TabularDataset\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "development = True\n",
    "use_cuda = True\n",
    "sample_data_folder = r'cnndm-pj/'\n",
    "\n",
    "MIN_LEN_X = 10\n",
    "MIN_LEN_Y = 10\n",
    "MAX_LEN_X = 400\n",
    "MAX_LEN_Y = 100\n",
    "MIN_NUM_X = 1\n",
    "MAX_NUM_X = 1\n",
    "MAX_NUM_Y = None\n",
    "W_LS = \"<s>\"\n",
    "W_RS = \"</s>\"\n",
    "SUMM_BEGIN_TOKEN = r\"-lrb- .* -rrb-\"\n",
    "PRETRAINED_VECTOR = 'glove.6B.200d' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "        torch.device(\"cuda\")\n",
    "        if use_cuda and torch.cuda.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TorchText to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize = \"spacy\",\n",
    "            tokenizer_language=\"en_core_web_sm\",\n",
    "            init_token = '<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tv_datafields = [(\"article\", TEXT), \n",
    "                 (\"summary\", TEXT)]\n",
    "trn, vld = TabularDataset.splits(\n",
    "               path=\"cnndm-pj\", # the root directory where the data lies\n",
    "               train='train_processed.txt', validation=\"val_processed.txt\",\n",
    "               format='csv',\n",
    "               skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields,\n",
    "                csv_reader_params ={'delimiter':'|'})\n",
    "\n",
    "tst_datafields = [(\"article\", TEXT), \n",
    "                 (\"summary\", TEXT)]\n",
    "tst = TabularDataset(\n",
    "           path=\"cnndm-pj/test_processed.txt\", # the file path\n",
    "           format='csv',\n",
    "           skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "           fields=tst_datafields,\n",
    "                csv_reader_params ={'delimiter':'|'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 samples from Train\n",
      "Removed 0 samples from Validation\n",
      "Removed 0 samples from Test\n",
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Removing incomplete examples, with no article or summary\n",
    "count = 0\n",
    "for ex in trn.examples:\n",
    "    if len(ex.__dict__.keys())!=2:\n",
    "        trn.examples.remove(ex)\n",
    "        count = count+1\n",
    "print (\"Removed {} samples from Train\".format(count))\n",
    "count =0\n",
    "for ex in vld.examples:\n",
    "    if len(ex.__dict__.keys())!=2:\n",
    "        vld.examples.remove(ex)\n",
    "        count = count +1\n",
    "print (\"Removed {} samples from Validation\".format(count))\n",
    "count = 0\n",
    "for ex in tst.examples:\n",
    "    if len(ex.__dict__.keys())!=2:\n",
    "        tst.examples.remove(ex)\n",
    "        count = count+1\n",
    "print (\"Removed {} samples from Test\".format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Building vocabulary on train only\n",
    "\n",
    "TEXT.build_vocab(trn, max_size=50000, min_freq=2,vectors=PRETRAINED_VECTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter = BucketIterator.splits(\n",
    " (trn, vld), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(64, 64),\n",
    " device=device, # if you want to use the GPU, specify the GPU number here\n",
    " sort_key=lambda x: len(x.article), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "test_iter = Iterator(tst, batch_size=64, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketDataLoader(BucketIterator):\n",
    "    def __init__(self, x_var, y_var, **kwargs):\n",
    "        self.x_var = x_var\n",
    "        self.y_var = y_var\n",
    "        super(BucketDataLoader, self).__init__(**kwargs)\n",
    "        \n",
    "    def batch(data, batch_size, batch_size_fn=None):\n",
    "        minibatch = super().batch(data, batch_size, batch_size_fn=batch_size_fn)\n",
    "        # we assume only one input in this wrapper\n",
    "        yield  getattr(minibatch, self.x_var), getattr(minibatch, self.y_var) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketDataLoader(dataset = trn, x_var = 'article', y_var='summary' ,batch_size=64,device = device, sort_key=lambda x: len(x.article), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = BucketDataLoader(dataset = vld, x_var = 'article', y_var='summary' ,batch_size=64,device = device, sort_key=lambda x: len(x.article), # the BucketIterator needs to be told what function it should use to group the data.\n",
    " sort_within_batch=False,\n",
    " repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "        def __init__(self, dl, x_var, y_var):\n",
    "            self.dl, self.x_var, self.y_var = dl, x_var, y_var # we pass in the list of attributes for x \n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                x = getattr(batch, self.x_var) # we assume only one input in this wrapper\n",
    "                y = getattr(batch, self.y_var) # we assume only one input in this wrapper\n",
    "                yield (x, y)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "train_dl = BatchWrapper(dl=train_iter, x_var = \"article\", y_var = \"summary\")\n",
    "valid_dl = BatchWrapper(val_iter, x_var = \"article\", y_var = \"summary\")\n",
    "test_dl = BatchWrapper(test_iter, x_var = \"article\", y_var = \"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "         [ 788,   33, 1391,  ..., 5702,   55,   33],\n",
       "         [2106,    4,    0,  ..., 5494,  254,    4],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [   1,    1,    1,  ...,    1,    1,    1]]),\n",
       " tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
       "         [  20,   20,   20,  ...,   20,   20,   20],\n",
       "         [ 460,    4, 1391,  ...,  128,   55,    4],\n",
       "         ...,\n",
       "         [   1,  483,    1,  ...,    1,    1,    1],\n",
       "         [   1,  362,    1,  ...,    1,    1,    1],\n",
       "         [   1,    3,    1,  ...,    1,    1,    1]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos>'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([416, 64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dl.__iter__())[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
